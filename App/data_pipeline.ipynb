{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "The goal of this notebook is to help you take a large PDF and prepare it to be queried by a large language model. This practice is called retreival augmented generation.\n",
    "\n",
    "Serves the following primary functions:\n",
    "\n",
    "* Extract data from PDF documents using Unstructured.io \n",
    "* Use data to create Llama Index nodes\n",
    "* Create vector embeddings for data and store in Pinecone Vector Database (you could sub in your own database here)\n",
    "* Create Index for data using Llama Index\n",
    "* Save index to local storage to use with your query engine (query engine provided in other files in this repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after installing packages from the requirements file import packages\n",
    "#IMPORTANT - unstructured with detectron (used for PDF parsing) does not run easily on Windows - recommend using Google Colab if you are window user\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from llama_index import GPTVectorStoreIndex, StorageContext, ServiceContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import pinecone\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'INSERT YOUR API KEY HERE'\n",
    "os.environ['PINECONE_API_KEY'] = 'INSERT YOUR API KEY HERE' #can create free pod (first is free) here https://www.pinecone.io/\n",
    "os.environ['PINECONE_ENVIRONMENT'] = 'gcp-starter' #update with the environment for your Pinecone Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from pdf with unstructured.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition_pdf(\"INSERT FILE NAME\", include_page_breaks=True) #more information on this here https://unstructured-io.github.io/unstructured/core/partition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Pages\n",
    "current_page = 1\n",
    "for element in elements:\n",
    "    if element.category == \"PageBreak\":\n",
    "        current_page += 1\n",
    "    else:\n",
    "        element.page_number = current_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Page Breaks (no longer necessary now that page numbers are added)\n",
    "elements = [element for element in elements if element.category != \"PageBreak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunk Text \n",
    "chunks = chunk_by_title(elements) #more information on this here https://unstructured-io.github.io/unstructured/core/chunking.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_raw = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Assuming each element has a 'text' attribute\n",
    "    # and using the filename and loop index as the doc id\n",
    "    # extra_info can be populated with any additional information you want from the element\n",
    "    doc_id = f\"{chunk.metadata.filename}_{i}\"\n",
    "    docs_raw.append(Document(\n",
    "        text=chunk.text,\n",
    "        doc_id=doc_id,\n",
    "        extra_info={\n",
    "            'page': chunk.page_number,\n",
    "            'filename': chunk.metadata.filename\n",
    "        }  # example, add more as needed\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SimpleNodeParser()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(docs_raw) #more info on nodes https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and/or Initialize Pinecone Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=os.environ['PINECONE_API_KEY'],\n",
    "    environment=os.environ['PINECONE_ENVIRONMENT']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'INSERT INDEX NAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional delete index code\n",
    "#pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create index if it doesn't exist\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pinecone.Index(index_name)\n",
    "namespace = '' # default namespace\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup our storage (vector db)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    ")\n",
    "# setup the index/query process, ie the embedding model (and completion if used)\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-ada-002', embed_batch_size=100)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(\n",
    "    docs_raw, storage_context=storage_context,\n",
    "    service_context=service_context\n",
    ")\n",
    "#more info on indexes here https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Index Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
